#!/usr/bin/env node

/**
 * LLMé›†æˆæµ‹è¯•
 * éªŒè¯å®Œæ•´çš„LLMä»£ç åˆ†ææµç¨‹
 */

import { AnalysisOrchestrator } from './app/lib/analysis-orchestrator'
import { GitHubClient } from './app/lib/github-client'
import { LLMClient } from './app/lib/llm-client'
import path from 'path'
import fs from 'fs-extra'

// é¢œè‰²è¾“å‡ºå·¥å…·
const colors = {
  green: '\x1b[32m',
  red: '\x1b[31m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  reset: '\x1b[0m'
}

function log(message: string, type: 'success' | 'error' | 'info' | 'warning' = 'info') {
  const colorMap = {
    success: colors.green,
    error: colors.red,
    info: colors.blue,
    warning: colors.yellow
  }
  console.log(`${colorMap[type]}${message}${colors.reset}`)
}

class LLMIntegrationTester {
  private testRepoPath: string

  constructor() {
    this.testRepoPath = path.join(process.cwd(), 'test-sample')
  }

  async runTests() {
    log('ğŸ¤– å¼€å§‹LLMé›†æˆæµ‹è¯•...', 'info')
    
    // æ£€æŸ¥APIå¯†é’¥
    const apiKey = process.env.LLM_API_KEY
    if (!apiKey) {
      log('âŒ æœªæ‰¾åˆ°LLM APIå¯†é’¥ï¼Œè¯·å…ˆè®¾ç½® LLM_API_KEY', 'error')
      return
    }

    await this.testLLMClient()
    await this.testAnalysisOrchestrator()
    await this.testFileAnalysis()
    await this.testProjectAnalysis()
    
    log('\nğŸ‰ LLMé›†æˆæµ‹è¯•å®Œæˆï¼', 'success')
  }

  async testLLMClient() {
    log('\nğŸ” æµ‹è¯•LLMå®¢æˆ·ç«¯...', 'info')
    
    const llmClient = new LLMClient({
      provider: process.env.LLM_PROVIDER as 'openai' | 'anthropic' || 'openai',
      apiKey: process.env.LLM_API_KEY || '',
      model: process.env.LLM_MODEL || 'gpt-4',
      baseURL: process.env.LLM_BASE_URL,
      maxTokens: 1000,
      temperature: 0.3
    })

    try {
      const testCode = `
function calculateSum(numbers) {
  let sum = 0;
  for (let i = 0; i < numbers.length; i++) {
    sum += numbers[i];
  }
  return sum;
}
      `

      const result = await llmClient.analyzeCode({
        code: testCode,
        filePath: 'test.js',
        language: 'JavaScript',
        analysisType: 'overview'
      })

      log(`âœ… LLMå®¢æˆ·ç«¯æµ‹è¯•é€šè¿‡: ${result.summary.slice(0, 100)}...`, 'success')
    } catch (error) {
      log(`âŒ LLMå®¢æˆ·ç«¯æµ‹è¯•å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`, 'error')
    }
  }

  async testAnalysisOrchestrator() {
    log('\nğŸ¯ æµ‹è¯•åˆ†æåè°ƒå™¨...', 'info')
    
    // åˆ›å»ºæµ‹è¯•é¡¹ç›®
    await this.createTestProject()
    
    const orchestrator = new AnalysisOrchestrator({
      llmConfig: {
        provider: process.env.LLM_PROVIDER as 'openai' | 'anthropic' || 'openai',
        apiKey: process.env.LLM_API_KEY || '',
        model: process.env.LLM_MODEL || 'gpt-4',
        baseURL: process.env.LLM_BASE_URL
      },
      analysisType: 'full',
      maxFilesToAnalyze: 5
    }, this.testRepoPath)

    try {
      const mockMetadata = {
        name: 'test-project',
        description: 'æµ‹è¯•é¡¹ç›®ç”¨äºLLMåˆ†æ',
        owner: 'test',
        stars: 100,
        language: 'TypeScript',
        topics: ['react', 'typescript', 'testing'],
        license: 'MIT',
        size: 1024,
        createdAt: new Date(),
        updatedAt: new Date()
      }

      const result = await orchestrator.analyzeRepository(
        this.testRepoPath,
        mockMetadata,
        (progress) => {
          log(`ğŸ“Š è¿›åº¦: ${progress.stage} - ${progress.progress}%`, 'info')
        }
      )

      log(`âœ… åˆ†æåè°ƒå™¨æµ‹è¯•é€šè¿‡:`, 'success')
      log(`   é¡¹ç›®: ${result.metadata.name}`, 'info')
      log(`   è¯­è¨€: ${result.metadata.language}`, 'info')
      log(`   å¤æ‚åº¦: ${result.codeQuality.complexity.average}`, 'info')
      log(`   æ´å¯Ÿ: ${result.llmInsights.architecture.slice(0, 100)}...`, 'info')

    } catch (error) {
      log(`âŒ åˆ†æåè°ƒå™¨æµ‹è¯•å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`, 'error')
    }
  }

  async testFileAnalysis() {
    log('\nğŸ“„ æµ‹è¯•æ–‡ä»¶åˆ†æ...', 'info')
    
    const testFile = path.join(this.testRepoPath, 'src', 'app.ts')
    
    const orchestrator = new AnalysisOrchestrator({
      llmConfig: {
        provider: process.env.LLM_PROVIDER as 'openai' | 'anthropic' || 'openai',
        apiKey: process.env.LLM_API_KEY || '',
        model: process.env.LLM_MODEL || 'gpt-4',
        baseURL: process.env.LLM_BASE_URL
      },
      analysisType: 'quality'
    }, this.testRepoPath)

    try {
      const fileAnalysis = await orchestrator.analyzeFile(testFile, 'complexity')
      
      log(`âœ… æ–‡ä»¶åˆ†ææµ‹è¯•é€šè¿‡:`, 'success')
      log(`   æ–‡ä»¶: ${testFile}`, 'info')
      log(`   æ€»ç»“: ${fileAnalysis.summary.slice(0, 100)}...`, 'info')
      log(`   è´¨é‡åˆ†æ•°: ${fileAnalysis.codeQuality.score}`, 'info')
      
    } catch (error) {
      log(`âŒ æ–‡ä»¶åˆ†ææµ‹è¯•å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`, 'error')
    }
  }

  async testProjectAnalysis() {
    log('\nğŸ—ï¸ æµ‹è¯•é¡¹ç›®åˆ†æ...', 'info')
    
    const llmClient = new LLMClient({
      provider: process.env.LLM_PROVIDER as 'openai' | 'anthropic' || 'openai',
      apiKey: process.env.LLM_API_KEY || '',
      model: process.env.LLM_MODEL || 'gpt-4',
      baseURL: process.env.LLM_BASE_URL,
      maxTokens: 1500
    })

    try {
      const projectPrompt = {
        projectStructure: JSON.stringify({
          src: ['app.ts', 'utils.ts', 'config.ts'],
          tests: ['app.test.ts'],
          package: 'package.json'
        }, null, 2),
        keyFiles: [{
          path: 'src/app.ts',
          content: `
import express from 'express'
import cors from 'cors'

const app = express()
app.use(cors())
app.use(express.json())

app.get('/', (req, res) => {
  res.json({ message: 'Hello World' })
})

export default app
          `.trim(),
          language: 'TypeScript'
        }],
        dependencies: ['express', 'cors', 'typescript'],
        entryPoints: ['src/app.ts'],
        analysisGoal: 'ç†è§£é¡¹ç›®æ¶æ„'
      }

      const projectAnalysis = await llmClient.analyzeProject(projectPrompt)
      
      log(`âœ… é¡¹ç›®åˆ†ææµ‹è¯•é€šè¿‡:`, 'success')
      log(`   æ¶æ„: ${projectAnalysis.architecture.slice(0, 100)}...`, 'info')
      log(`   æŠ€æœ¯æ ˆ: ${projectAnalysis.technologyStack.join(', ')}`, 'info')
      log(`   å…³é”®åŠŸèƒ½: ${projectAnalysis.keyFeatures.slice(0, 2).join(', ')}`, 'info')
      
    } catch (error) {
      log(`âŒ é¡¹ç›®åˆ†ææµ‹è¯•å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`, 'error')
    }
  }

  async createTestProject() {
    await fs.ensureDir(this.testRepoPath)
    await fs.ensureDir(path.join(this.testRepoPath, 'src'))
    
    const packageJson = {
      name: 'test-project',
      version: '1.0.0',
      description: 'æµ‹è¯•é¡¹ç›®ç”¨äºLLMåˆ†æ',
      main: 'src/app.ts',
      dependencies: {
        express: '^4.18.0',
        cors: '^2.8.5'
      },
      devDependencies: {
        typescript: '^5.0.0',
        '@types/express': '^4.17.0'
      },
      scripts: {
        start: 'node dist/app.js',
        dev: 'ts-node src/app.ts'
      }
    }

    await fs.writeJson(path.join(this.testRepoPath, 'package.json'), packageJson, { spaces: 2 })
    
    const appTs = `
import express from 'express'
import cors from 'cors'

interface User {
  id: number
  name: string
  email: string
}

class AppServer {
  private app: express.Application
  private port: number

  constructor(port: number) {
    this.app = express()
    this.port = port
    this.initializeMiddleware()
    this.initializeRoutes()
  }

  private initializeMiddleware() {
    this.app.use(cors())
    this.app.use(express.json())
  }

  private initializeRoutes() {
    this.app.get('/', (req, res) => {
      res.json({ message: 'Hello World' })
    })

    this.app.get('/users', (req, res) => {
      const users: User[] = [
        { id: 1, name: 'Alice', email: 'alice@example.com' },
        { id: 2, name: 'Bob', email: 'bob@example.com' }
      ]
      res.json(users)
    })
  }

  public start() {
    this.app.listen(this.port, () => {
      console.log(\`Server running on port \${this.port}\`)
    })
  }
}

const server = new AppServer(3000)
server.start()
    `.trim()

    await fs.writeFile(path.join(this.testRepoPath, 'src', 'app.ts'), appTs)
    await fs.writeFile(path.join(this.testRepoPath, 'README.md'), '# Test Project\n\nThis is a test project for LLM analysis.')
  }

  async cleanup() {
    await fs.remove(this.testRepoPath)
  }
}

// è¿è¡Œæµ‹è¯•
async function runLLMTests() {
  try {
    const tester = new LLMIntegrationTester()
    await tester.runTests()
    await tester.cleanup()
  } catch (error) {
    log(`æµ‹è¯•è¿è¡Œå¤±è´¥: ${error instanceof Error ? error.message : String(error)}`, 'error')
    process.exit(1)
  }
}

// å¦‚æœç›´æ¥è¿è¡Œ
if (require.main === module) {
  runLLMTests().catch(console.error)
}

export { LLMIntegrationTester }